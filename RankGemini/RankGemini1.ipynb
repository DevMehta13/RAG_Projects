{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xujm4ueRXl2K",
        "outputId": "02cff6a8-73a6-4afa-98cc-b4beba4df551"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/rank_llm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ntmI20mZBvV",
        "outputId": "c6612ca4-93c7-41d8-cfec-163b78442a36"
      },
      "outputs": [],
      "source": [
        "!pip install -q -e \"./rank_llm[genai]\" langchain langchain-google-genai langchain-community langchain-core cohere gitpython rank_bm25 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WId89a-eavBa",
        "outputId": "fe28e52c-fb16-4f48-888b-f2b26455a102"
      },
      "outputs": [],
      "source": [
        "!pip list | grep -E 'rank-llm|langchain|faiss|dotenv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "-pEZ4s_PZEqi",
        "outputId": "3599473d-8363-4c85-b4a5-00c8c7237d0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from google.colab import userdata\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Failed to load GPU Faiss\")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "from langchain_community.document_loaders import GitLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "import google.generativeai as genai\n",
        "from rank_llm.rerank.rankllm import PromptMode\n",
        "from rank_llm.rerank.listwise.rank_gemini import SafeGenai\n",
        "from rank_llm.data import Query, Candidate, Request\n",
        "\n",
        "\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "print(\"Initializing all AI components...\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    google_api_key=google_api_key\n",
        ")\n",
        "\n",
        "genai.configure(api_key=google_api_key)\n",
        "\n",
        "\n",
        "native_gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "reranker = SafeGenai(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    context_size=8192,\n",
        "    keys=[google_api_key],\n",
        "    prompt_mode=PromptMode.RANK_GPT_APEER,\n",
        ")\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "repo_path = \"/tmp/requests_repo\"\n",
        "if os.path.exists(repo_path):\n",
        "    shutil.rmtree(repo_path)\n",
        "\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/Nikhils-G/Agentic-RAG-Chatbot\",\n",
        "    repo_path=\"/tmp/requests_repo\",\n",
        "    branch=\"main\",\n",
        "    file_filter=lambda file_path: file_path.endswith(\".py\")\n",
        ")\n",
        "row_docs = loader.load()\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "chunks = python_splitter.split_documents(row_docs)\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "bm25_retriever.k = 20\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[vector_retriever, bm25_retriever], weights=[0.5, 0.5], search_type=\"mmr\"\n",
        ")\n",
        "\n",
        "prompt_template_str = \"\"\"You are an expert programming assistant.\n",
        "Your job is to answer questions about a Python codebase.\n",
        "Use the following retrieved and reranked code snippets to answer the user's question.\n",
        "If you don't know the answer from the context provided, just say that you don't know.\n",
        "Do not make up an answer. Provide code examples from the context where relevant.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "def ask_with_reranker(question: str):\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    retrieved_lc_docs = ensemble_retriever.invoke(question)\n",
        "\n",
        "    if not retrieved_lc_docs:\n",
        "        print(\"\\nâœ… No documents retrieved.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    query = Query(text=question, qid=1)\n",
        "    candidates_to_rerank = retrieved_lc_docs[:100]\n",
        "    rank_llm_candidates = [\n",
        "        Candidate(docid=f\"doc_{i}\", score=0.0, doc={\"text\": lc_doc.page_content, \"metadata\": lc_doc.metadata})\n",
        "        for i, lc_doc in enumerate(candidates_to_rerank)\n",
        "    ]\n",
        "    single_request = Request(query=query, candidates=rank_llm_candidates)\n",
        "\n",
        "\n",
        "    batch_results = reranker.rerank_batch(requests=[single_request], top_k=5)\n",
        "    rerank_results = batch_results[0]\n",
        "\n",
        "    print(\"      4. Generating final answer...\")\n",
        "    reranked_docs_content = [result.doc[\"text\"] for result in rerank_results.candidates]\n",
        "    context_str = \"\\n\\n---\\n\\n\".join(reranked_docs_content)\n",
        "    final_prompt = prompt_template_str.format(context=context_str, question=question)\n",
        "    native_response = native_gemini_model.generate_content(final_prompt)\n",
        "    response = native_response.text\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(response)\n",
        "\n",
        "\n",
        "    print(\"Original order (first 3):\")\n",
        "    for i in range(min(3, len(candidates_to_rerank))):\n",
        "        source = candidates_to_rerank[i].metadata.get('source', 'N/A')\n",
        "        print(f\"  {i+1}. doc_{i} - {source}\")\n",
        "\n",
        "    print(\"Reranked order (first 3):\")\n",
        "    for i in range(min(3, len(rerank_results.candidates))):\n",
        "        candidate_id = rerank_results.candidates[i].docid\n",
        "        # Extract original index from docid (e.g., \"doc_5\" -> 5)\n",
        "        original_idx = int(candidate_id.split('_')[1])\n",
        "        source = rerank_results.candidates[i].doc[\"metadata\"].get('source', 'N/A')\n",
        "        print(f\"  {i+1}. {candidate_id} (was #{original_idx+1}) - {source}\")\n",
        "\n",
        "\n",
        "my_question = \"Parser?\"\n",
        "ask_with_reranker(my_question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
